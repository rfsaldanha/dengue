---
title: "Denque and weather lags"
author: Raphael Saldanha
date: last-modified
---

This notebook aims to study the relationship between Dengue cases incidence with lagged climate indicators, specially the co-occurrence of specific climate conditions that precedes an outbreak.

## Packages

```{r}
#| message: false
library(tidyverse)
library(tidymodels)
library(hotspots)
library(DT)
library(finetune)
library(bonsai)
library(themis)
library(arrow)
library(timetk)
library(rpart.plot)
library(vip)
library(doParallel)
doParallel::registerDoParallel()

source("../functions.R")
```

## Dataset construction

### Dengue

```{r}
code_muni <- 3136702 # Juiz de Fora, MG
```

The original data on municipality dengue cases incidence present daily observations and is summarized by month.

```{r}
dengue_df <- open_dataset(data_dir("dengue_data/parquet_aggregated/dengue_md.parquet")) %>%
  filter(mun == !!substr(code_muni, 0, 6)) %>%
  collect() %>%
  summarise_by_time(.date_var = date, .by = "month", freq = sum(freq, na.rm = TRUE))
```

#### Classify

Based on the observed frequency distribution of cases, we classify the months as anomalous or not.

```{r}
hot_ref <- hotspots(
  x = dengue_df$freq, 
  p = 0.99, 
  var.est = "mad",
)$positive.cut

dengue_df_anom <- dengue_df %>%
  mutate(anomaly = if_else(freq >= hot_ref, "Yes", "No")) %>%
  mutate(anomaly = as.factor(anomaly))
```

```{r}
dengue_df_anom %>%
  ggplot(aes(x = date, y = freq, fill = anomaly)) +
  geom_bar(stat = "identity") +
  scale_fill_discrete(direction = -1) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal"
  )
```

```{r}
dengue_df <- inner_join(dengue_df, dengue_df_anom) 
```

Proportion table of anomalous (yes) and not anomalous (not) months.

```{r}
prop.table(table(dengue_df$anomaly))
```

### Weather data

The available weather data is also originally presented in daily observations and aggregate to months. For temperature indicators, the mean is used, for precipitation, the sum is used for aggregation.

```{r}
tmax <- open_dataset(sources = data_dir("weather_data/parquet/brdwgd/tmax.parquet")) %>%
  filter(code_muni == code_muni) %>%
  filter(name == "Tmax_mean") %>%
  select(date, value) %>%
  collect() %>%
  filter(date >= min(dengue_df$date) & date <= max(dengue_df$date)) %>%
  summarise_by_time(.date_var = date, .by = "month", value = mean(value, na.rm = TRUE)) %>%
  rename(tmax = value)

tmin <- open_dataset(sources = data_dir("weather_data/parquet/brdwgd/tmin.parquet")) %>%
  filter(code_muni == code_muni) %>%
  filter(name == "Tmin_mean") %>%
  select(date, value) %>%
  collect() %>%
  filter(date >= min(dengue_df$date) & date <= max(dengue_df$date)) %>%
  summarise_by_time(.date_var = date, .by = "month", value = mean(value, na.rm = TRUE)) %>%
  rename(tmin = value)

prec <- open_dataset(sources = data_dir("weather_data/parquet/brdwgd/pr.parquet")) %>%
  filter(code_muni == code_muni) %>%
  filter(name == "pr_sum") %>%
  select(date, value) %>%
  collect() %>%
  filter(date >= min(dengue_df$date) & date <= max(dengue_df$date)) %>%
  summarise_by_time(.date_var = date, .by = "month", value = sum(value, na.rm = TRUE)) %>%
  rename(prec = value)
```

### Join data

Join dengue and weather datasets.

```{r}
res <- inner_join(x = dengue_df, y = tmax, by = "date") %>%
  inner_join(tmin, by = "date") %>%
  inner_join(prec, by = "date") %>%
  select(date, anomaly, tmax, tmin, prec)
```

### Time lag

This step produces time lagged variables (from 1 to 6 months) from the weather indicators, remove the date variable, and omit records with missing data (only present after the time lag procedure).

```{r}
res_prep <- res %>%
  select(-date) %>%
  tk_augment_lags(
    .value = c(tmax, tmin, prec), 
    .lags = 1:6
  ) %>% 
  select(-tmax, -tmin, -prec) %>%
  na.omit()
```

```{r}
head(res_prep) %>% datatable()
```

### Dataset split

Splits the dataset into training and testing.

```{r}
res_split <- initial_time_split(
  data = res_prep, 
  prop = .8, 
  lag = 6
)

train_data <- training(res_split)
test_data <- testing(res_split)
```

Remove old objects and triggers a memory garbage collection.

```{r}
rm(dengue_df, dengue_df_anom, res, res_prep)
gc()
```

## Modeling

### Recipes

Creates model recipes with the model specitication, data (train dataset). Several recipes are created with different methods to balance the training dataset.

```{r}
rec_upsample <- 
  recipe(anomaly ~ ., train_data) %>%
  step_upsample(
    anomaly, 
    over_ratio = tune()
  )
```

```{r}
rec_rose <- 
  recipe(anomaly ~ ., train_data) %>%
  step_rose(
    anomaly, 
    over_ratio = tune()
  )
```

```{r}
rec_smote <- 
  recipe(anomaly ~ ., train_data) %>%
  step_smote(
    anomaly, 
    over_ratio = tune(),
    neighbors = tune()
  )
```

```{r}
rec_adasyn <- 
  recipe(anomaly ~ ., train_data) %>%
  step_adasyn(
    anomaly, 
    over_ratio = tune(),
    neighbors = tune()
  )
```

### Learners

Decision trees are choose due its directly interpretability and rules extraction. Two learners are created with different engines (rpart and partykit).

```{r}
tree_rp_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

```{r}
tree_pk_spec <- decision_tree(
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("partykit") %>%
  set_mode("classification")
```

### Folding

Creates a v-fold for cross-validation.

```{r}
folds <- vfold_cv(
  data = train_data, 
  v = 10, 
  strata = anomaly
)
```

### Workflow setting

This step creates an modeling workflow by combining the recipes and learners options.

```{r}
wf_set <- 
  workflow_set(
    preproc = list(
      upsample = rec_upsample,
      rose = rec_rose, 
      smote = rec_smote, 
      adasyn = rec_adasyn
    ),
    models = list(
      dt_rp = tree_rp_spec, 
      dt_pk = tree_pk_spec
    ),
    cross = TRUE
  )
```

### Tuning

This step tune hyper-parameters from the models (learners and balancing steps) using an ANOVA race.

```{r}
tune_results <- wf_set %>%
  workflow_map(
    "tune_race_anova",
    seed = 345,
    resamples = folds,
    grid = 50,
    control = control_race(parallel_over = "everything"),
    verbose = TRUE
  )
```

### Best workflow and model selection

Based on the tuning results, this step identifies the best learner strategy and best model hyper-parameters based on the ROC-AUC metric.

```{r}
best_wf <- tune_results %>% 
  rank_results(rank_metric = "roc_auc") %>% 
  filter(.metric == "roc_auc") %>% 
  select(wflow_id, model, .config, accuracy = mean, rank) %>%
  slice(1) %>%
  pull(wflow_id)

print(best_wf)
```

```{r}
best_tune <- tune_results %>%
  extract_workflow_set_result(id = best_wf) %>%
  select_best(metric = "roc_auc")

t(best_tune)
```

### Finalize workflow

Finalizes the workflow with the choose learner and hyper-parameter combination, performing the last fit of the model with the entire dataset.

```{r}
fitted_wf <- tune_results %>%
  extract_workflow(id = best_wf) %>%
  finalize_workflow(best_tune) %>%
  last_fit(split = res_split)
```

## Results

### Decision tree plot

```{r}
extracted_engine <-  fitted_wf %>% extract_fit_engine()

if(class(extracted_engine)[1] == "constparty"){
  plot(extracted_engine)
} else (
  rpart.plot(extracted_engine, roundint = FALSE)
)
  
```

### Confusion matrix

```{r}
augment(fitted_wf) %>%
  conf_mat(truth = anomaly, estimate = .pred_class)
```

### Model performance metrics

```{r}
augment(fitted_wf) %>%
  metrics(truth = anomaly, estimate = .pred_class, .pred_Yes)
```

### Variable importance plot

```{r}
fitted_wf %>% 
  extract_fit_engine() %>% 
  vip()
```

## Session info

```{r}
sessionInfo()
```
